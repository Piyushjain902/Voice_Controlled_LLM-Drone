{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxbrbCCyV6Kc"
      },
      "source": [
        "***Installation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2sbWDgApiPh",
        "outputId": "f02a5453-8144-4b1e-9157-835dec83c41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m696.3/800.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-whisper\n",
        "!sudo apt-get install -y ffmpeg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P3mK4ksV_vL"
      },
      "source": [
        "**Taking Voice Command from User And Converting into Raw Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W5FaXjgsxRA",
        "outputId": "20e98cb9-9308-4237-d3a7-50f8f67c636a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Transcription:  Drone, take off and after that move forward and then hold there.\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Transcribe using Whisper\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe audio file\n",
        "result = whisper_model.transcribe(\"/content/QuadCommandOne.m4a\")\n",
        "raw_text = result[\"text\"]\n",
        "\n",
        "print(\"Raw Transcription:\", raw_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsRCBFfCWCPw"
      },
      "source": [
        "# Refining Raw text using flan-t5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLCMo7orWCMP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WT1kdj6tHqZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCcko84vuIuL",
        "outputId": "4a284dde-43bc-4819-86a2-a8c8a1f30e3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cleaned Text: Drone, take off and after that move forward and then hold there.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load Flan-T5\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the correction prompt\n",
        "from transformers import pipeline\n",
        "corrector = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "instruction = f\"the following text has come from speech to text coversion where speech is related to drone,correct the raw text:\\n{raw_text}\"\n",
        "\n",
        "# Get response\n",
        "result = corrector(instruction, max_length=100)\n",
        "cleaned_text = result[0]['generated_text']\n",
        "\n",
        "print(\"✅ Cleaned Text:\", result[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_VTFanSWMM8"
      },
      "source": [
        "# Setting up openai api key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_bHOLgUuXen",
        "outputId": "68d17484-da9e-4e5a-cf90-73bcd3c357dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMOnUAKrx5-C"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=\"My API KEY\")  # Replace with your real API key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivbSffJ0WQV_"
      },
      "source": [
        "# Generating code from text in accordance with necessary prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPpnSx01zw0L"
      },
      "outputs": [],
      "source": [
        "def generate_mavsdk_code(command_text):\n",
        "    prompt = f\"\"\"\n",
        "You are an expert in MAVSDK Python library for drone programming . Convert the following natural language instruction into valid MAVSDK Python code.\n",
        "\n",
        "Instruction:\n",
        "\"{command_text}\"\n",
        "\n",
        "Your response should be a complete, working script using MAVSDK, using async/await, drone connection, arming, flight steps, and return to home if needed. Output only code.Remember to execute instuctions only.\n",
        "In order to move drone in forward,backward,left,right ,up,down direction use this :\n",
        "   print(\"-- Setting initial setpoint\")\n",
        "    await drone.offboard.set_velocity_body(\n",
        "        VelocityBodyYawspeed(0.0, 0.0, 0.0, 0.0))\n",
        "\n",
        "    print(\"-- drone attitude control\")\n",
        "    await drone.offboard.set_velocity_body(\n",
        "        VelocityBodyYawspeed(0.0, 0.0, 0.0, 0.0))\n",
        "    await asyncio.sleep(5)\n",
        "\n",
        "in this the parameters are in order (Vx,Vy,Vz,yaw velocity).\n",
        "\n",
        "in order to execute a circle use :\n",
        "\"    print(\"-- Fly a circle\")\n",
        "    await drone.offboard.set_velocity_body(\n",
        "        VelocityBodyYawspeed(5.0, 0.0, 0.0, 30.0))\n",
        "    await asyncio.sleep(5)\n",
        "\"\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # 🧠 Top-tier model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOXzxchA3yTr",
        "outputId": "491e9ac9-b6e0-4d6f-a1fb-06a443d07e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚁 Generated MAVSDK Code:\n",
            "\n",
            "```python\n",
            "import asyncio\n",
            "from mavsdk import System\n",
            "from mavsdk.offboard import VelocityBodyYawspeed\n",
            "\n",
            "async def run():\n",
            "    drone = System()\n",
            "    await drone.connect(system_address=\"udp://:14540\")\n",
            "\n",
            "    print(\"Waiting for drone to connect...\")\n",
            "    async for state in drone.core.connection_state():\n",
            "        if state.is_connected:\n",
            "            print(f\"Drone discovered!\")\n",
            "            break\n",
            "\n",
            "    print(\"-- Arming\")\n",
            "    await drone.action.arm()\n",
            "\n",
            "    print(\"-- Taking off\")\n",
            "    await drone.action.takeoff()\n",
            "    await asyncio.sleep(5)\n",
            "\n",
            "    print(\"-- Setting initial setpoint\")\n",
            "    await drone.offboard.set_velocity_body(\n",
            "        VelocityBodyYawspeed(0.0, 0.0, 0.0, 0.0))\n",
            "\n",
            "    print(\"-- Starting offboard\")\n",
            "    try:\n",
            "        await drone.offboard.start()\n",
            "    except Exception as e:\n",
            "        print(f\"Failed to start offboard mode: {e}\")\n",
            "        return\n",
            "\n",
            "    print(\"-- Moving forward\")\n",
            "    await drone.offboard.set_velocity_body(\n",
            "        VelocityBodyYawspeed(5.0, 0.0, 0.0, 0.0))\n",
            "    await asyncio.sleep(5)\n",
            "\n",
            "    print(\"-- Holding position\")\n",
            "    await drone.offboard.set_velocity_body(\n",
            "        VelocityBodyYawspeed(0.0, 0.0, 0.0, 0.0))\n",
            "    await asyncio.sleep(5)\n",
            "\n",
            "    print(\"-- Stopping offboard\")\n",
            "    try:\n",
            "        await drone.offboard.stop()\n",
            "    except Exception as e:\n",
            "        print(f\"Failed to stop offboard mode: {e}\")\n",
            "\n",
            "    print(\"-- Returning to launch\")\n",
            "    await drone.action.return_to_launch()\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    loop = asyncio.get_event_loop()\n",
            "    loop.run_until_complete(run())\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "final_code = generate_mavsdk_code(cleaned_text)\n",
        "print(\"🚁 Generated MAVSDK Code:\\n\")\n",
        "print(final_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzw314iNWUNO"
      },
      "source": [
        "# The above code is tested on quadcopter in simulation\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
